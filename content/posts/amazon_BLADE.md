---
title: '✏️ 비대칭을 해결하기 위한 아마존의 무기, BLADE'
date: '2023-10-06'
categories: ['GNN', 'Deep Learning', 'RecSys']
showToc: true
ShowBreadCrumbs: true
comments : true
draft : false
---

## 🐣 Vanilla Phone

나는 스마트폰 케이스를 따로 끼질 않는다. 스마트폰 액정 필름도 마찬가지다. 이런 `바닐라 스마트폰`을 보면 대다수는 우려 섞인 시선과 함께 "떨어뜨려서 폰이 깨지면 어떡하나요?"하고 걱정하곤 한다. 그럴 때마다 "저는 잘 안 떨어뜨려요."하고 웃으며 넘긴다. 실제로 폰을 잘 안 떨어뜨리기도 하지만 한편으로는 스마트폰 자체의 견고함을 믿는 마음도 있다. 설령 떨어뜨려서 상처가 나면 어떠하리. 고치면 되는 것을. 물론 그보다도, 그냥 어떠한 케이스도 씌우지 않은 스마트폰 본연의 모습이 가장 아름답다는 이유가 가장 크다.

그럼에도 가끔은 쇼핑몰에서 스마트폰 케이스를 구경할 때가 있다. "음 요즘은 스마트폰에 요란한 스트랩을 매다는 게 유행이군." 하며 트렌드 분석가 마냥 흐름을 파악하려는 건 아니고, 케이스 자체가 예쁜 브랜드들을 종종 발견하면 구매하고 싶은 욕구가 들기도 하니까. 케이스를 구경하다가 또 다른 추천 목록에 뜬 부가 상품들을 구경하다 보면 시간이 가는 줄 모른다.

![](/images/recsys/phone.webp)

스마트폰 케이스를 재밌게 구경하던 와중에 추천 상품에 아이폰 혹은 갤럭시 스마트폰 불쑥 뜬다면 아마도 뜬금없다는 느낌을 받을 것이다. 왜냐하면 나는 이미 스마트폰이 있으니까 케이스나 필름을 고르는 터인데, 스마트폰을 추천한다고? 이건 굳이 필요 없는 행동일 수 있다. 반면에 내가 아이폰, 갤럭시를 구경하다가 추천 상품에 관련 케이스가 뜨는 건 이상하지 않을 것이다. 합리적인 추천이라는 생각에 아이쇼핑을 신나게 하겠지. 즉 스마트폰과 스마트폰 케이스는 서로 연관되어 있는 제품이지만 그 관계는 동등하지 않다. 이른바 `asymmetric relationship`이다.

제품을 추천하는 데 있어서 이런 `asymmetric relationship`은 골칫거리 중 하나다. 이렇게 비대칭적인 연관 제품 관계를 추천 시스템에서 해결하기 위해 등장한 논문 하나를 가지고 왔다.


## 🐣 RecSys 2023

우선 논문을 소개하기 앞서서 지난 9월 18일부터 22일까지 싱가포르에서 개최된 RecSys 2023 이야기부터 하겠다. 이름에서 알 수 있듯 추천 시스템 분야의 새로운 연구 결과와 기술을 발표하는 포럼인데, ACM에서 개최하고 있다. ACM은 1947년에 설립된 세계 최대 규모의 컴퓨터 과학 분야의 국제 학회다. 올해의 RecSys는 싱가포르의 선텍시티 컨벤션 센터에서 개최됐다. 크고 웅장한 규모를 자랑한다.

![](/images/recsys/suntec.jpeg)

3일 동안 세 번에 걸쳐서 기조연설이 있었는데, 세 번 중 두 번의 주제가 LLM일만큼 추천 시스템에서도 LLM은 큰 관심을 받고 있다. 우선 첫날은 Microsoft의 수석과학자 제이미 티반(Jamie Teevan)이 어떻게 LLM이 우리 업무에 영향을 미치는지, LLM이 업무 생산성과 창의성에 어떤 영향을 주는지에 대해 설명하였다. 둘째 날 기조연설을 담당한 건 싱가포르 국립대학교의 KITHCT 석좌교수인 추아 박사였다. 그는 LLM 기반 서비스가 가지고 있는 현재의 문제점, 이를테면 신뢰성 문제 등을 개선해서 최종적으로 어떻게 Generative 검색과 추천 서비스에 다다를 수 있는지에 대해 이야기했다.

오늘 이야기할 `asymmetri relationship`에 대한 이야기는 마지막 날 기조연설에서 확인할 수 있다. 이 날의 기조연설은 Amazon International Emerging Stores 부의 응용과학담당 부사장 라지브 라스토기(Rajeev Rastogi)가 했는데, 그는 연설에서 아마존이 추천 알고리즘 작업에서 직면한 세 가지 문제에 대해 이야기하였다. 세 가지 문제는 다음과 같다.

1. 방향성 그래프에서의 추천 시스템 문제
2. 시간이 지남에 따라 대상 레이블이 변경될 경우 머신러닝 모델 훈련의 문제
3. 예측 불확실성 추정치를 활용해 모델의 정확도를 개선하는 문제

`asymmetri relationship`는 바로 첫 번째 문제와 연관이 있다.


## 🐣 Directed Graphs

![](/images/recsys/undirected.png)

그래프는 매우 다양한 종류의 그래프가 존재한다. 하지만 크게 분류하자면 방향성 그래프(Directed Graph)와 비방향성 그래프(Undirected Graph)로 나눌 수 있다. 두 그래프의 차이점은 엣지의 유형이다. 이름에서 알 수 있듯 방향성 그래프는 엣지에 방향성이 부여된다. 하나의 노드에서 또 다른 노드로. 반면 비방향성 그래프는 두 노드를 연결해 줄 뿐 노드 간의 관계나 방향성을 부여하진 않는다.

앞서 예를 들었던 스마트폰과 스마트폰 케이스를 그래프로 표현한다고 생각해 보자. 가령 스마트폰 케이스를 노드 A로 놓고 스마트폰을 노드 B로 두고, 해당 아이템을 추천할 경우에 엣지로 연결한다고 해보겠다. 스마트폰(B)이 입력값이라면 케이스(A)를 추천하는 건 합리적이다. 반면 케이스(A)가 입력값인 상태에서 스마트폰(B)을 추천하는 건 쉽지 않다. 그래프로 표시하면 B에서 A로 나가는 엣지를 그릴 수 있을 것이다. 또 다른 스마트폰(C), 충전기 어댑터(D), 보호 필름(E) 등... 조금 더 추천 시스템을 확장해서 그래프로 표현한다면 아래와 같은 방향성 그래프를 얻을 수 있겠다.

![](/images/recsys/undirected_BLADE.png)

추천 서비스라는 건 이렇게 그래프로 표현된 데이터 상에서 엣지의 존재 유무를 예측하는 Link Prediction Task와 같다. 이걸 해결하기 위해 여러 GNN 알고리즘이 있지만 문제는 대부분이 비방향성 그래프를 활용한 단일 임베딩 공간을 통한 다는 것이다. 앞서 살펴봤듯 대부분의 추천에서 연관 제품 간의 관계는 대칭적이지 않다. 아마존은 기존의 GNN을 활용해 모델링을 하기엔 비대칭 관계를 표현하기 부족하다고 인식하고, 그 한계를 해결하기 위한 알고리즘을 고안했다. 바로 BLADE다.

## 🐣 BLADE

BLADE는 Biased Locally Adaptive Direction awarE라는 이름을 가진 알고리즘이다. BLADE는 단일 임베딩의 한계를 극복하기 위해 이중 임베딩을 학습한다. 위에 그렸던 제품 그래프를 가지고 기존의 GNN과 아마존의 BLADE가 어떻게 다르게 임베딩을 학습하는지 살펴보자.

![](/images/recsys/BLADE_graph.webp)

왼쪽은 기존 그래프 신경망에서 노드 A의 임베딩 계산 그래프이다. 노드 A의 정보는 A와 연결된 연결된 노드 B, C, E의 정보를 합치고, 또 그 노드에 연결된 정보를 가져오는 식으로 모을 수 있을 것이다. 일반적인 MPNN(Message Passing Neural Network) 구조다.

반면 BLADE에서는 비대칭적 구조를 나타내기 위해 임베딩을 이중으로 진행한다. 하나는 추천 대상으로, 또 하나는 추천 소스로. 오른쪽 위에 표시된 계산 그래프는 A를 추천 소스로 바라봤을 때의 임베딩, A-s(source)이다. 아래에 표시된 계산 그래프는 A가 추천 대상일 경우의 임베딩, 즉 A-t(target)이다. 소스 임베딩을 생성할 때는 노드에서 나가는 이웃을, 타겟 임베딩을 생성할 땐 들어오는 이웃으로 표현하면 방향성을 구분할 수 있다.

아마존에서는 BLADE를 이용해 모든 노드에 대해서 소스 및 타겟 임베딩 생성한 다음엔 기존의 추천 시스템과 동일한 과정을 거치고 있다. 우리의 목표는 상품이 주어졌을 때, 그 상품과 구매될 가능성이 높은 상위 제품이 무엇인지를 찾는 것. 즉, 쿼리 상품 Q를 임베딩 공간에 매핑하고, 잠재적 후보를 타겟 임베딩 공간에 매핑한 뒤 추천 노드를 찾기 위해 최근접 이웃 탐색을 진행한다. 그 과정이 아래 알고리즘 1에 나타나있다.

![](/images/recsys/BLADE_algorithm.png)

알고리즘 1은 BLADE의 노드 임베딩 생성을 담당한다. 하지만 요 알고리즘은 모델이 이미 훈련되어 있는 경우에 사용할 수 있다. 즉 각 layer의 가중치 매트릭스 $W$가 존재하는 경우에 이중 임베딩(source, target)을 생성하는 알고리즘인 셈이다. 그렇다면 가중치 매트릭스 $W$는 어떻게 구할까? 바로 알고리즘 2에 풀이법이 있다. 알고리즘 2는 BLADE의 파라미터를 학습하는 과정을 담당한다.

사실 BLADE에서 핵심은 알고리즘 2라고 할 수 있다. 보통의 바닐라 GNN에선 모든 노드에 대해 고정된 이웃 크기를 샘플링하고, 대부분의 GNN 선행연구들이 high-degree 노드를 기반으로 연구되어왔다. 하지만 이론과 현실은 그리 가깝지 않은 법. 아마존처럼 대규모의 상품을 다루는 커머스는 물론 대부분의 현실 그래프에서는 high-degree 노드에 비해 low-degree 노드가 훨씬 더 많다. 이른바 멱함수(Power-Law) 분포, 롱테일 법칙이다.

아마존에선 이 부분의 최적화를 위해 adaptive sampling 기술을 적용했다. 시드 노드의 degree에 따라 이웃 샘플의 크기를 locally 조정하는 거다. 만약 시드 노드의 degree가 낮으면 이웃 샘플링을 더 크게, degree가 높으면 이웃 샘플링을 더 적게 하는 식으로 말이다. 이 논리는 BLADE(Biased Locally Adaptive Direction awarE)의 이름에 오롯이 담겨 있다. 

물론 GAT(Graph Attention)를 활용하면 노드마다 다른 가중치를 암시적으로 지정할 수 있다. 하지만 Attention 가중치는 노드의 이웃 feature를 기반으로 계산되는지라 그래프의 구조적 특성을 반영할 수 없다는 한계가 있다. 하지만 저자들은 BLADE가 이 한계를 극복한다고 설명한다.

## 📚 Reference

1. [Srinivas Virinchi and Anoop Saladi. BLADE: Biased Neighborhood Sampling based Graph Neural Network for Directed Graphs.(2023)](https://dl.acm.org/doi/abs/10.1145/3539597.3570430)
2. [RecSys: Rajeev Rastogi on three recommendation system challenges](https://www.amazon.science/blog/recsys-rajeev-rastogi-on-three-recommendation-system-challenges)